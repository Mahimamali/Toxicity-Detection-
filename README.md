# Toxicity-Detection
Social media and online communities struggle to deal with increasing hate speech, abuse, and cyberbullying, toxic text detection has become a crucial Natural language processing challenge. In this, we propose and compare two deep learning models for better text toxicity detection: Long Short-Term Memory (LSTM) and Bi-Directional Gated Recurrent Unit (GRU) neural networks, along with GloVe and FastText embeddings. We also implemented ensemble techniques on the two top performing models. We evaluated the performance of our proposed models on the widely used dataset, the Jigsaw Toxic Comment Classification Challenge. We achieved an accuracy of 92.41% after combining both Bi-GRU with GloVe embeddings and Bi-GRU with FastText embeddings using model averaging ensemble techniques. We also encountered some research gaps while training the models: Biases affecting the models out of which algorithmic bias has a strong influence on what is considered abuse or slurs and even making inaccurate and unfair predictions, another gap is the lack of proper training data affecting the detection of toxicity in conversations where the language is dynamic and changing.

We have also published a research paper on the same.
If you like to know more about the research carried out, this is the link of the research paper: https://ijirt.org/Article?manuscript=160891
Link of the dataset used:https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/overview
